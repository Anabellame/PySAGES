# SPDX-License-Identifier: GPL-3.0-or-later
# Copyright (c) 2020-2021: PySAGES contributors
# See LICENSE.md and CONTRIBUTORS.md at https://github.com/SSAGESLabs/PySAGES

"""
Adaptive Biasing Force (ABF) sampling method.

ABF partitions the collective variable space into bins determined by a user
provided grid, and keeps a tabulation of the number of visits to each bin
as well as the sum of generalized forces experienced by the system at each
configuration bin. These provide an estimate for the mean generalized force,
which can be integrated to yield the free energy.

The implementation of the adaptive biasing force method here closely follows
https://doi.org/10.1063/1.2829861. One important difference is that the time
derivative of the product :math:`W\\cdot p` (equation 9 of reference) is approximated by a
second order backward finite difference in the simulation timestep.
"""

from typing import NamedTuple

from jax import numpy as np, scipy
from jax import grad, jit, vmap

from pysages.grids import build_indexer
from pysages.methods.core import GriddedSamplingMethod, generalize, Result
from pysages.utils import JaxArray

# $$ for analysis
from pysages.utils import dispatch
from functools import partial
from pysages.approxfun.core import compute_mesh, scale as _scale
from pysages.grids import build_indexer
from pysages.methods.core import NNSamplingMethod, generalize
from pysages.ml.models import MLP, Siren
from pysages.ml.objectives import GradientsSSE, L2Regularization
from pysages.ml.optimizers import LevenbergMarquardt
from pysages.ml.training import NNData, build_fitting_function, normalize, convolve
from pysages.ml.utils import blackman_kernel, pack, unpack
from pysages.utils import Bool, Int, JaxArray


class ABFState(NamedTuple):
    """
    ABF internal state.

    Parameters
    ----------
    bias : JaxArray (Nparticles, 3)
        Array with biasing forces for each particle.
    xi : JaxArray (CV shape)
        Last collective variable recorded in the simulation.
    hist: JaxArray (grid.shape)
        Histogram of visits to the bins in the collective variable grid.
    force_sum: JaxArray (grid.shape, CV shape)
        Cumulative forces at each bin in the CV grid.
    force: JaxArray (grid.shape, CV shape)
        Average force at each bin of the CV grid.
    Wp: JaxArray (CV shape)
        Product of W matrix and momenta matrix for the current step.
    Wp_: JaxArray (CV shape)
        Product of W matrix and momenta matrix for the previous step.
    """

    bias: JaxArray
    xi: JaxArray
    hist: JaxArray
    Fsum: JaxArray
    force: JaxArray
    Wp: JaxArray
    Wp_: JaxArray

    def __repr__(self):
        return repr("PySAGES " + type(self).__name__)


class ABF(GriddedSamplingMethod):
    """
    Constructor of the ABF method.

    Parameters
    ----------
    self : ABF
        See parent class
    cvs:
        See parent class
    args:
        See parent class
    kwargs:
        An optional keyword argument `N: int` can be specified as
        threshold parameter before accounting for the full average
        of the adaptive biasing force  (defaults to 200).

    Attributes
    ----------
    snapshot_flags
      Indicating the fields required in a snapshot.
    """

    snapshot_flags = {"positions", "indices", "momenta"}

    def __init__(self, cvs, grid, *args, **kwargs):
        super().__init__(cvs, grid, *args, **kwargs)
        self.N = np.asarray(self.kwargs.get("N", 200))

    def build(self, snapshot, helpers, *args, **kwargs):
        """
        Build the functions for the execution of ABF

        Arguments
        ---------
        snapshot:
            PySAGES snapshot of the simulation (backend dependent).
        helpers:
            Helper function bundle as generated by
            `SamplingMethod.context[0].get_backend().build_helpers`.

        Returns
        -------
        Tuple `(snapshot, initialize, update)` to run ABF simulations.
        """
        return _abf(self, snapshot, helpers)


def _abf(method, snapshot, helpers):
    """
    Internal function that generates the init and update functions.

    Arguments
    ---------
    method: ABF
        Class that generates the functions.
    snapshot:
        PySAGES snapshot of the simulation (backend dependent).
    helpers
        Helper function bundle as generated by
        `SamplingMethod.context[0].get_backend().build_helpers`.

    Returns
    -------
    Tuple `(snapshot, initialize, update)` to run ABF simulations.
    """
    cv = method.cv
    grid = method.grid
    N = method.N

    dt = snapshot.dt
    dims = grid.shape.size
    natoms = np.size(snapshot.positions, 0)
    get_grid_index = build_indexer(grid)

    def initialize():
        """
        Internal function that generates the first ABFstate with correctly shaped JaxArrays.

        Returns
        -------
        ABFState
            Initialized State
        """
        bias = np.zeros((natoms, 3))
        hist = np.zeros(grid.shape, dtype=np.uint32)
        Fsum = np.zeros((*grid.shape, dims))
        force = np.zeros(dims)
        Wp = np.zeros(dims)
        Wp_ = np.zeros(dims)
        return ABFState(bias, None, hist, Fsum, force, Wp, Wp_)

    def update(state, data):
        """
        Advance the state of the ABF simulation.

        Arguments
        ---------
        state: ABFstate
            Old ABFstate from the previous simutlation step.
        data: JaxArray
            Snapshot to access simulation data.

        Returns
        -------
        ABFState
            Updated internal state.
        """
        # Compute the collective variable and its jacobian
        xi, Jxi = cv(data)

        p = data.momenta
        # The following could equivalently be computed as `linalg.pinv(Jxi.T) @ p`
        # (both seem to have the same performance).
        # Another option to benchmark against is
        # Wp = linalg.tensorsolve(Jxi @ Jxi.T, Jxi @ p)
        Wp = scipy.linalg.solve(Jxi @ Jxi.T, Jxi @ p, sym_pos="sym")
        # Second order backward finite difference
        dWp_dt = (1.5 * Wp - 2.0 * state.Wp + 0.5 * state.Wp_) / dt

        I_xi = get_grid_index(xi)
        N_xi = state.hist[I_xi] + 1
        # Add previous force to remove bias
        force_xi = state.Fsum[I_xi] + dWp_dt + state.force
        hist = state.hist.at[I_xi].set(N_xi)
        Fsum = state.Fsum.at[I_xi].set(force_xi)
        force = force_xi / np.maximum(N_xi, N)

        bias = np.reshape(-Jxi.T @ force, state.bias.shape)

        return ABFState(bias, xi, hist, Fsum, force, Wp, state.Wp)

    return snapshot, initialize, generalize(update, helpers)


@dispatch
def analyze(result: Result[ABF], **kwargs):
    """
    Computes the free energy from the result of an `ABF` run.
    Integrates the forces using Sobolev approximation of functions by gradient learning.
    Arguments
    ---------
    result: Result[ABF]: Result bundle containing method,
           final abf state, and callback.

    Keyword arguments
    -----------------
    topology: Tuple[int] (default (4, 4))
        Defines the architecture of the neural network
        (number of nodes of each hidden layer).
    Returns
    -------
    dict: A ``dict`` with the following keys:

        mean_forces:JaxArray (grid.shape, CV shape)
            Average force at each bin of the CV grid.
        free_energy:JaxArray (grid.shape)
            Free Energy obtained from mean_forces using described method above
        cvfit:JaxArray (grid.shape)
            Grid used in the method
        histogram:JaxArray (grid.shape)
            Histogram for the states visited during the method


    """
    topology = kwargs.get("topology", (4, 4))
    method = result.method
    grid = method.grid
    state = result.states

    def _learn_forces(train, train_freq, ps, state):
        hist = np.expand_dims(state.hist, state.hist.ndim)
        F = state.Fsum / np.maximum(hist, 1)
        return train(state.nn, F)

    class AnalysisState(NamedTuple):
        hist: JaxArray
        Fsum: JaxArray
        nn: NNData

    def train_result(fit, smooth, inputs, nn, y):
        axes = tuple(range(y.ndim - 1))
        std = y.std(axis=axes).max()
        reference = smooth(y / std)
        params = fit(nn.params, inputs, reference).params
        return NNData(params, nn.mean, std / reference.std(axis=axes).max())

    scale = partial(_scale, grid=grid)
    dims = grid.shape.size
    model = MLP(dims, 1, topology, transform=scale)
    ps, layout = unpack(model.parameters)

    optimizer = LevenbergMarquardt(loss=GradientsSSE(), max_iters=2500, reg=L2Regularization(1e-3))
    fit = build_fitting_function(model, optimizer)
    inputs = (compute_mesh(grid) + 1) * grid.size / 2 + grid.lower
    w = 5 if type(model) is Siren else 7
    smooth = partial(
        convolve, kernel=blackman_kernel(dims, w), boundary="wrap" if grid.is_periodic else "edge"
    )
    train = jit(partial(train_result, fit, lambda y: vmap(smooth)(y.T).T, inputs))
    learn_forces = jit(partial(_learn_forces, train, 1, ps))

    mesh = compute_mesh(grid)
    x = grid.size * (mesh + 1) / 2 + grid.lower

    # %%
    def average_forces(state):
        Fsum = state.Fsum
        shape = (*Fsum.shape[:-1], 1)
        return Fsum / np.maximum(state.hist.reshape(shape), 1)

    mean_forces = average_forces(state)
    analysis_state = AnalysisState(state.hist, state.Fsum, NNData(ps, 0.0, 1.0))
    nn = learn_forces(analysis_state)
    A = nn.std * model.apply(pack(nn.params, layout), x) + nn.mean
    A = (A.max() - A).reshape(grid.shape)
    return dict(mean_forces=mean_forces, free_energy=A, cvfit=x, histogram=state.hist)
